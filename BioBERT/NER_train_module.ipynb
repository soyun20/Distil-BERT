{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437ea841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "#from transformers import AutoTokenizer\n",
    "from transformers import DistilBertForTokenClassification\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score\n",
    "import re\n",
    "from torchcrf import CRF\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "#import part\n",
    "\n",
    "using_biobert = False #biobert option\n",
    "using_bilstmcrf = True #biLSTM + CRF option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2fb9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 43821 #columns of dataset\n",
    "df = pd.read_csv(\"NER_dataset.csv\", encoding=\"cp949\").sample(frac=1)[:N] #load dataset\n",
    "\n",
    "#change field name\n",
    "df.rename(columns = {'text':'sentence', 'labels':'tags'}, inplace = True)\n",
    "\n",
    "#split train, dev, test data (dev data is used for cross validation)\n",
    "df_train, df_dev, df_test = np.split(df.sample(frac=1, random_state=0), [int(.8 * len(df)), int(.9 * len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc68dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40113</th>\n",
       "      <td>AZEL HIBBSKAYS , M.D. CF66</td>\n",
       "      <td>O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6458</th>\n",
       "      <td>2) Azotemia presumed secondary to sepsis and d...</td>\n",
       "      <td>O B-PR O O O B-PR O B-PR O B-TE O O O O B-TR I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22986</th>\n",
       "      <td>White count was 14.4 , hematocrit was 36.2 , p...</td>\n",
       "      <td>B-TE I-TE O O O B-TE O O O B-TE I-TE O O O B-T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25131</th>\n",
       "      <td>There was artificial rupture of membranes was ...</td>\n",
       "      <td>O O B-TR I-TR I-TR I-TR O O O B-TR I-TR O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35653</th>\n",
       "      <td>( End of Report )</td>\n",
       "      <td>O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "40113                         AZEL HIBBSKAYS , M.D. CF66   \n",
       "6458   2) Azotemia presumed secondary to sepsis and d...   \n",
       "22986  White count was 14.4 , hematocrit was 36.2 , p...   \n",
       "25131  There was artificial rupture of membranes was ...   \n",
       "35653                                  ( End of Report )   \n",
       "\n",
       "                                                    tags  \n",
       "40113                                         O O O O O   \n",
       "6458   O B-PR O O O B-PR O B-PR O B-TE O O O O B-TR I...  \n",
       "22986  B-TE I-TE O O O B-TE O O O B-TE I-TE O O O B-T...  \n",
       "25131         O O B-TR I-TR I-TR I-TR O O O B-TR I-TR O   \n",
       "35653                                         O O O O O   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "#show head part of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa71c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33065</th>\n",
       "      <td>2013-05-24 07:40 AM 628 *</td>\n",
       "      <td>O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7792</th>\n",
       "      <td>Angioectasias in the fundus</td>\n",
       "      <td>B-PR I-PR I-PR I-PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17931</th>\n",
       "      <td>BP 157/85</td>\n",
       "      <td>B-TE O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>At 9am the morning of admission he passed a la...</td>\n",
       "      <td>O O O O O O O O B-PR I-PR I-PR I-PR I-PR I-PR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33842</th>\n",
       "      <td>Heart Failure Service was involved .</td>\n",
       "      <td>O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "33065                          2013-05-24 07:40 AM 628 *   \n",
       "7792                         Angioectasias in the fundus   \n",
       "17931                                          BP 157/85   \n",
       "24     At 9am the morning of admission he passed a la...   \n",
       "33842               Heart Failure Service was involved .   \n",
       "\n",
       "                                                    tags  \n",
       "33065                                         O O O O O   \n",
       "7792                                B-PR I-PR I-PR I-PR   \n",
       "17931                                            B-TE O   \n",
       "24     O O O O O O O O B-PR I-PR I-PR I-PR I-PR I-PR ...  \n",
       "33842                                       O O O O O O   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()\n",
    "#show tail part of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7e26e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-TR', 'I-TE', 'O', 'I-PR', 'B-TE', 'B-PR', 'B-TR'}\n",
      "{'B-PR': 0, 'B-TE': 1, 'B-TR': 2, 'I-PR': 3, 'I-TE': 4, 'I-TR': 5, 'O': 6}\n"
     ]
    }
   ],
   "source": [
    "# tansfrom label to list (delimiter is \" \")\n",
    "labels = [i.split() for i in df['tags'].values.tolist()]\n",
    "\n",
    "# count the number of label\n",
    "unique_labels = set()\n",
    "\n",
    "for lb in labels:\n",
    "    [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
    " \n",
    "print(unique_labels)\n",
    "# {'B-TE', 'I-TR', 'I-PR', 'I-TE', 'O', 'B-TR', 'B-PR'}\n",
    "\n",
    "# mappingg unique_lables to their id\n",
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "print(labels_to_ids)\n",
    "# {'B-PR': 0, 'B-TE': 1, 'B-TR': 2, 'I-PR': 3, 'I-TE': 4, 'I-TR': 5, 'O': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da4c180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main class for doing NER with Distilbert\n",
    "\n",
    "class DistilbertNER(nn.Module):\n",
    "  \n",
    "    def __init__(self, tokens_dim): #constructor \n",
    "        \n",
    "        super(DistilbertNER,self).__init__()\n",
    "        #constructor of parent\n",
    "    \n",
    "        if type(tokens_dim) != int:\n",
    "            raise TypeError('tokens_dim should be an integer')\n",
    "        if tokens_dim <= 0:\n",
    "            raise ValueError('Classification layer dimension should be at least 1')\n",
    "        #exception handling part\n",
    "            \n",
    "        if using_biobert == False :        \n",
    "            self.pretrained = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = tokens_dim) #for using distilBERT model\n",
    "        else :\n",
    "            self.pretrained = AutoModelForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", num_labels = tokens_dim) #for using bioBERT model\n",
    "       \n",
    "        if using_bilstmcrf == True :\n",
    "            hidden_size=768\n",
    "            num_classes=7 #7 labels\n",
    "            dr_rate=0.3\n",
    "    \n",
    "            self.dropout = nn.Dropout(dr_rate)\n",
    "            self.bilstm = nn.LSTM(hidden_size, (hidden_size) // 2, dropout=dr_rate, batch_first=True, bidirectional=True)\n",
    "            self.position_wise_ff = nn.Linear(hidden_size, num_classes)\n",
    "            self.crf = CRF(num_tags=num_classes, batch_first = True)\n",
    "    \n",
    "            #set the output of each token classifier = unique_lables\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels = None): #define the way for getting ouput by given input\n",
    "        if labels == None:\n",
    "            out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask )\n",
    "            return out #return without label\n",
    "\n",
    "        else: #labels != None\n",
    "            out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask , labels = labels)\n",
    "            return out #return with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3cac910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generalized function for get information from dataset\n",
    "\n",
    "class NerDataset(torch.utils.data.Dataset):\n",
    "  \n",
    "  def __init__(self, df):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "      raise TypeError('Input should be a dataframe')\n",
    "    \n",
    "    if \"tags\" not in df.columns or \"sentence\" not in df.columns:\n",
    "      raise ValueError(\"Dataframe should contain 'tags' and 'sentence' columns\")\n",
    "    \n",
    "    tags_list = [i.split() for i in df[\"tags\"].values.tolist()]\n",
    "    texts = df[\"sentence\"].values.tolist()\n",
    "    \n",
    "    #for change float(nan) -> string(\"nan\")\n",
    "    i = 0\n",
    "    for string in texts:\n",
    "        i += 1\n",
    "        if(isinstance(string, float)):\n",
    "            texts[i - 1] = \"nan\"\n",
    "\n",
    "    self.texts = [tokenizer(text, padding = \"max_length\", truncation = True, return_tensors = \"pt\") for text in texts]\n",
    "    self.labels = [match_tokens_labels(text, tags) for text,tags in zip(self.texts, tags_list)]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_text = self.texts[idx]\n",
    "    batch_labels = self.labels[idx]\n",
    "\n",
    "    return batch_text, torch.LongTensor(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e4bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracking():\n",
    "  def __init__(self):\n",
    "\n",
    "    self.total_acc = 0\n",
    "    self.total_f1 = 0\n",
    "    self.total_precision = 0\n",
    "    self.total_recall = 0\n",
    "\n",
    "  def update(self, predictions, labels , ignore_token = -100):  \n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    predictions = predictions[labels != ignore_token]\n",
    "    labels = labels[labels != ignore_token]\n",
    "\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    labels = labels.to(\"cpu\")\n",
    "\n",
    "    acc = accuracy_score(labels,predictions)\n",
    "    f1 = f1_score(labels, predictions, average = \"macro\")\n",
    "    precision = precision_score(labels, predictions, average = \"macro\")\n",
    "    recall = recall_score(labels, predictions, average = \"macro\")\n",
    "\n",
    "    self.total_acc  += acc\n",
    "    self.total_f1 += f1\n",
    "    self.total_precision += precision\n",
    "    self.total_recall  += recall\n",
    "\n",
    "  def return_avg_metrics(self,data_loader_size):\n",
    "    n = data_loader_size\n",
    "    metrics = {\n",
    "        \"acc\": round(self.total_acc / n ,3), \n",
    "        \"f1\": round(self.total_f1 / n, 3), \n",
    "        \"precision\" : round(self.total_precision / n, 3), \n",
    "        \"recall\": round(self.total_recall / n, 3)\n",
    "          }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f8f46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create label\n",
    "\n",
    "def tags_2_labels(tags : str, tag2idx : dict):\n",
    "  return [tag2idx[tag] if tag in tag2idx else unseen_label for tag in tags.split()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16653275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map words to tag\n",
    "\n",
    "def tags_mapping(tags_series : pd.Series):\n",
    "  if not isinstance(tags_series, pd.Series):\n",
    "      raise TypeError('Input should be a padas Series')\n",
    "\n",
    "  unique_tags = set()\n",
    "  \n",
    "  for tag_list in df_train[\"tags\"]:\n",
    "    for tag in tag_list.split():\n",
    "      unique_tags.add(tag)\n",
    "\n",
    "  tag2idx = {k:v for v,k in enumerate(sorted(unique_tags))}\n",
    "  idx2tag = {k:v for v,k in tag2idx.items()}\n",
    "\n",
    "  unseen_label = tag2idx[\"O\"]\n",
    "\n",
    "  return tag2idx, idx2tag, unseen_label, unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66e276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-100 means CLS or PAD\n",
    "\n",
    "def match_tokens_labels(tokenized_input, tags, ignore_token = -100):\n",
    "        word_ids = tokenized_input.word_ids()\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "\n",
    "            if word_idx is None:\n",
    "                label_ids.append(ignore_token)\n",
    "\n",
    "            else :\n",
    "                try:\n",
    "                  reference_tag = tags[word_idx]\n",
    "                  label_ids.append(tag2idx[reference_tag])\n",
    "                except:\n",
    "                  label_ids.append(ignore_token)\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6845bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train & evaluation function\n",
    "\n",
    "def train_loop(model, train_dataset, dev_dataset, optimizer,  batch_size, epochs):\n",
    "  \n",
    "  train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "  dev_dataloader = DataLoader(dev_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model = model.to(device)\n",
    "\n",
    "  for epoch in range(epochs) : \n",
    "    \n",
    "    train_metrics = MetricsTracking()\n",
    "    total_loss_train = 0\n",
    "\n",
    "    model.train() #core function for train\n",
    "\n",
    "    for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "      train_label = train_label.to(device)\n",
    "\n",
    "      mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "      input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      output = model(input_id, mask, train_label)\n",
    "      loss, logits = output.loss, output.logits\n",
    "      predictions = logits.argmax(dim= -1) \n",
    "\n",
    "      train_metrics.update(predictions, train_label)\n",
    "      total_loss_train += loss.item()\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    \n",
    "    model.eval() #core function for evaluation\n",
    "\n",
    "    dev_metrics = MetricsTracking()\n",
    "    total_loss_dev = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for dev_data, dev_label in dev_dataloader:\n",
    "\n",
    "        dev_label = dev_label.to(device)\n",
    "\n",
    "        mask = dev_data['attention_mask'].squeeze(1).to(device)\n",
    "        input_id = dev_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "        output = model(input_id, mask, dev_label)\n",
    "        loss, logits = output.loss, output.logits\n",
    "\n",
    "        predictions = logits.argmax(dim= -1)     \n",
    "\n",
    "        dev_metrics.update(predictions, dev_label)\n",
    "        total_loss_dev += loss.item()\n",
    "    \n",
    "    train_results = train_metrics.return_avg_metrics(len(train_dataloader))\n",
    "    dev_results = dev_metrics.return_avg_metrics(len(dev_dataloader))\n",
    "\n",
    "    print(f\"TRAIN \\nLoss: {total_loss_train / len(train_dataset)} \\nMetrics {train_results}\\n\" ) \n",
    "    print(f\"VALIDATION \\nLoss {total_loss_dev / len(dev_dataset)} \\nMetrics{dev_results}\\n\" )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ed86fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mapp between label and tag\n",
    "tag2idx, idx2tag , unseen_label, unique_tags = tags_mapping(df_train[\"tags\"])\n",
    "\n",
    "#change label to tag (surplus label will be changed to \"O\" tag)\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "  df[\"labels\"] = df[\"tags\"].apply(lambda tags : tags_2_labels(tags, tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35fd3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_biobert == False :\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") #needed to useing distilBERT model\n",
    "else :\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", model_max_length = 512) #needed to useing bioBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22643578",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_train[\"sentence\"].values.tolist()\n",
    "\n",
    "#for change float(nan) -> string(\"nan\")\n",
    "\n",
    "i = 0\n",
    "for string in text:\n",
    "    i += 1\n",
    "    if(isinstance(string, float)):\n",
    "        text[i - 1] = \"nan\"\n",
    "\n",
    "text_tokenized = tokenizer(text, padding = \"max_length\", truncation = True, return_tensors = \"pt\")\n",
    "#text_tokenized = text_tokenized.remove_columns(books_dataset[\"train\"].column_names) #needed to useing bioBERT tokenizer\n",
    "\n",
    "#map tokens to corresponding words\n",
    "word_ids = text_tokenized.word_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b79c747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL is not exist so new MODEL will be created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilbertNER(\n",
       "  (pretrained): DistilBertForTokenClassification(\n",
       "    (distilbert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (bilstm): LSTM(768, 384, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (position_wise_ff): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (crf): CRF(num_tags=7)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilbertNER(len(unique_tags))\n",
    "learn = False\n",
    "\n",
    "#determine whether new train & learn is needed or not\n",
    "\n",
    "try :\n",
    "    model = torch.load(\"NER_model\", map_location=torch.device('cpu'))\n",
    "except FileNotFoundError as e : \n",
    "    print(\"MODEL is not exist so new MODEL will be created\")\n",
    "    learn = True\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbdd320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2191/2191 [07:32<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Loss: 0.029421302068907137 \n",
      "Metrics {'acc': 0.839, 'f1': 0.609, 'precision': 0.689, 'recall': 0.604}\n",
      "\n",
      "VALIDATION \n",
      "Loss 0.02195437626920841 \n",
      "Metrics{'acc': 0.877, 'f1': 0.704, 'precision': 0.785, 'recall': 0.686}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2191/2191 [07:37<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Loss: 0.019063821510254098 \n",
      "Metrics {'acc': 0.892, 'f1': 0.747, 'precision': 0.811, 'recall': 0.736}\n",
      "\n",
      "VALIDATION \n",
      "Loss 0.017555131884757217 \n",
      "Metrics{'acc': 0.9, 'f1': 0.768, 'precision': 0.813, 'recall': 0.771}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2191/2191 [07:37<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Loss: 0.014570196061179387 \n",
      "Metrics {'acc': 0.916, 'f1': 0.808, 'precision': 0.856, 'recall': 0.8}\n",
      "\n",
      "VALIDATION \n",
      "Loss 0.016656909990543096 \n",
      "Metrics{'acc': 0.908, 'f1': 0.79, 'precision': 0.829, 'recall': 0.791}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2191/2191 [07:35<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Loss: 0.01214027072993519 \n",
      "Metrics {'acc': 0.929, 'f1': 0.838, 'precision': 0.88, 'recall': 0.831}\n",
      "\n",
      "VALIDATION \n",
      "Loss 0.01603966121149935 \n",
      "Metrics{'acc': 0.916, 'f1': 0.809, 'precision': 0.838, 'recall': 0.818}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2191/2191 [07:34<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Loss: 0.010589892369773538 \n",
      "Metrics {'acc': 0.937, 'f1': 0.856, 'precision': 0.89, 'recall': 0.85}\n",
      "\n",
      "VALIDATION \n",
      "Loss 0.017373026489135215 \n",
      "Metrics{'acc': 0.911, 'f1': 0.794, 'precision': 0.849, 'recall': 0.784}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set the hyperparameters\n",
    "\n",
    "train_dataset = NerDataset(df_train)\n",
    "dev_dataset = NerDataset(df_dev)\n",
    "\n",
    "lr = 1e-2\n",
    "optimizer = SGD(model.parameters(), lr=lr, momentum = 0.9)  \n",
    "\n",
    "#MAIN\n",
    "\n",
    "if using_biobert == False :\n",
    "    parameters = {\n",
    "        \"model\": model,\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"dev_dataset\" : dev_dataset,\n",
    "        \"optimizer\" : optimizer,\n",
    "        \"batch_size\" : 16,\n",
    "        \"epochs\" : 5\n",
    "    }\n",
    "else :\n",
    "    parameters = {\n",
    "        \"model\": model,\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"dev_dataset\" : dev_dataset,\n",
    "        \"optimizer\" : optimizer,\n",
    "        \"batch_size\" : 8, #to avoid out of memory error\n",
    "        \"epochs\" : 5\n",
    "    }\n",
    "\n",
    "if learn == True: #do train & test if NER_model not exist\n",
    "    train_loop(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52be4953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if learn == True: #do export the result of train if NER_model not exist\n",
    "    torch.save(model, \"NER_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
